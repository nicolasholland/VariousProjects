{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving MNIST with different python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different machine learning packages out there. We want to have a look at the most popular ones and see how we can use them to solve a well known problem. MNIST has become the \"hello world\" of machine learning. It is a dataset containing images of handwritten digits and labels and the goal is to train a model that can classify the images correctly. In this notebook we use scikit-learn and Tensor Flow and we are especially interested in the necessary steps to get from 'I have data' to 'I have a trained model'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a python packge built on NumPy and SciPy. It contains easy to use implementations of many machine learning algorithms and is one of the most used machine learning packages in python.\n",
    "To solve our problem we use a support vector machine implementation from scikit-learn and train it on MNIST data.\n",
    "The following MNIST example is an abbreviated version taken from Scikit-learns documentation.\n",
    "For the full example look here: http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.735555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, svm, metrics, model_selection\n",
    "\n",
    "# Prepare data\n",
    "data, target = datasets.load_digits(return_X_y=True)\n",
    "train_data, test_data, train_target, expected = model_selection.train_test_split(data, target, random_state=1)\n",
    "\n",
    "# create and train classifier\n",
    "classifier = svm.SVC(gamma=0.01)\n",
    "classifier.fit(train_data, train_target)\n",
    "\n",
    "predicted = classifier.predict(test_data)\n",
    "print(\"Accuracy score\", metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using scikit-learn (and machine learning in general) we often have to make decisions that influence how good our models can be. We have to decide what algorithm to use, what model to use or what model parameters to use. These problems have lead to researchers looking into how to automate machine learning itself. Think of it as 'We want to learn how to learn\". There is an interesting package called auto-sklearn which is build on top of scikit-learn. In our first example we decided to use a support vector machine with an input parameter. When we run the same example again, but with auto-sklearn the auto classifier will choose an algorithm and input parameters for us. The example is from https://automl.github.io/auto-sklearn/stable/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.988888888889\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "automl.fit(train_data, train_target)\n",
    "predicted = automl.predict(test_data)\n",
    "print(\"Accuracy score\", metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we want to have a look at Tensor Flow. Tensor Flow is an open source software that was originally developed by Google. Its popularity has many reasons one of which is, that it is suited for both research as well as production environments. We wont go into detail since it is a very large and powerfull software library. Instead we define a simple model for MNIST training and look at the necessary steps and to train it. \n",
    "In contrast to scikit-learn, where we simply wrote \"model.fit\" we now have to declare our model and train it explicitly using gradient descent in a for loop. \n",
    "The code we use is again an abbreviated version from a tutorial we found on their documentation. For the full tutorial look here: http://www.tensorflow.org/get_started/mnist/beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9065\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# init variables\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# implement model\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# Train model\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = data.train.next_batch(100)\n",
    "    sess.run(tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy), feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy score\", sess.run(accuracy, feed_dict={x: data.test.images, y_: data.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
